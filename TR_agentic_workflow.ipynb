{
 "metadata": {
  "kernelspec": {
   "display_name": "snow_tru_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "lastEditStatus": {
   "notebookId": "4tuq7f72dq5lt5syjajc",
   "authorId": "5095547476787",
   "authorName": "EBOTWICK",
   "authorEmail": "elliott.botwick@snowflake.com",
   "sessionId": "80c2bbd1-e3fb-4f53-b124-35a5f439b58f",
   "lastEditTime": 1743699734957
  }
 },
 "nbformat_minor": 2,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "dbec7150-dfc0-47d5-a10e-f05d5895760e",
   "metadata": {
    "language": "python",
    "name": "imports"
   },
   "outputs": [],
   "source": "import streamlit as st\nimport pandas as pd\nimport numpy as np\nfrom faker import Faker\nimport random\nimport json\n\nfrom snowflake.cortex import complete\nfrom snowflake.snowpark.context import get_active_session\n\nsession = get_active_session()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a265202c-9003-4bcb-8b72-a6735ca9094a",
   "metadata": {
    "language": "python",
    "name": "generate_dataset",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "#Synthetically create a dataframe of customers, products, contract val, and received revenue\n\ndef generate_customer_data():\n    \"\"\"Generates mock customer data.\"\"\"\n\n    data = []\n    product_list =[\"Cortex LLM\",\"Cortex Search\",\n                   \"Cortex Analyst\",\"Cortex Agents\",\n                \"DocAI\",\"Model Registry\"]\n\n\n    customer_list = [\"Apple\", \"Amazon\", \"Microsoft\", \"Alphabet\", \"Google\", \"Facebook\", \"Procter & Gamble\", \"Coca-Cola\", \n                         \"ExxonMobil\", \"Verizon\", \"Johnson & Johnson\", \"Pfizer\", \"UnitedHealth Group\", \"Caterpillar\", \n                         \"3M\", \"Chevron\", \"Walmart\", \"McDonald's\", \"AT&T\", \"Berkshire Hathaway\", \"Sears Holdings\", \"Cargill\", \"DuPont\", \n                         \"General Electric\", \"Merck\", \"Visa\", \"Intel\", \"Cisco Systems\", \"Wealthsimple\", \"Snowflake\"]\n        \n    for i in customer_list:\n        customer_name = i\n        products_used = random.sample(product_list\n,\n            random.randint(1, 4),\n        )\n        selected_product = random.sample(product_list, 1)[0]\n        total_contract_value = round(random.uniform(1000, 100000), 2)\n        revenue_received = round(random.uniform(0, total_contract_value), 2)\n\n        data.append(\n            {\n                \"CUSTOMER\": customer_name,\n                \"TOTAL_CONTRACT_VALUE\": total_contract_value,\n                \"REVENUE_RECEIVED\": revenue_received,\n                \"PRODUCTS_USED\": products_used,\n                \"PROPENSITY_MODEL_SUGGESTED_PRODUCT\": selected_product,\n\n            }\n        )\n    df = pd.DataFrame(data)\n    return df\n\n# Generate and display the DataFrame\ndf = generate_customer_data()\ndf.tail(10)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "97ce10d6-35e8-429c-93a2-26559beb239e",
   "metadata": {
    "language": "python",
    "name": "write_data_to_snowflake"
   },
   "outputs": [],
   "source": "# session.write_pandas(df, \"CUSTOMER_DATA\", auto_create_table=True, quote_identifiers = False, overwrite=False)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5e401c13-7f55-4da9-9846-b2da6b8c23ba",
   "metadata": {
    "language": "sql",
    "name": "show_tables"
   },
   "outputs": [],
   "source": "SHOW TABLES;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "778a1d23-ddb6-4b8b-aee4-1feb7aef72b0",
   "metadata": {
    "language": "sql",
    "name": "create_stage"
   },
   "outputs": [],
   "source": "CREATE STAGE IF NOT EXISTS SEMANTIC DIRECTORY = ( ENABLE = true );",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4720f417-0bdf-4990-b36e-99f0a96c9c58",
   "metadata": {
    "name": "create_semantic_model_via_UI",
    "collapsed": false
   },
   "source": "# We will now create the semantic model to query our new data with natural language\n\n* To do so, first go to the AI Studio in snowsight and click the Cortex Analyst Tab\n\n* Choose the appropriate database and schema the select the SEMANTIC stage we just created\n\n* Click Create New\n\n* Fill out the Description - \n    * \"Semantic model containing information about customer product data including the customer name, which products they currently use, their total contract value, and the amount of revenue we have received from the customer\"\n\n* Select the CUSTOMER_PRODUCT_DATA table and select all columns\n"
  },
  {
   "cell_type": "code",
   "id": "22d4c5b0-b88e-4109-91c9-0d5b99553428",
   "metadata": {
    "language": "python",
    "name": "load_data"
   },
   "outputs": [],
   "source": "data = session.table(\"CUSTOMER_DATA\")\ndata.show(5)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5998dc6b-f680-48fd-9d30-6644541e98ac",
   "metadata": {
    "language": "python",
    "name": "define_cortex_analyst_class"
   },
   "outputs": [],
   "source": "## Start testing cortex analyst api call here\nfrom typing import List, Dict, Optional\nimport _snowflake\n\n\nclass CortexAnalyst():\n    def __init__(self, db: str, schema: str, stage: str, semantic_model_file_path: str):\n        self.db = db\n        self.schema = schema\n        self.stage = stage\n        self.semantic_model_file_path = semantic_model_file_path\n\n\n    # @instrument\n    def send_message(self, prompt: str) -> dict:\n\n        \"\"\"Calls the REST API and returns the response.\"\"\"\n        \n        messages = []\n        messages.append({\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prompt}]})\n        request_body = {\n            \"messages\": messages, #need to wrap in a list?\n            \"semantic_model_file\": f\"@{self.db}.{self.schema}.{self.stage}/{self.semantic_model_file_path}\",\n        }\n\n        print(request_body)\n\n        resp = _snowflake.send_snow_api_request(\n            \"POST\",\n            f\"/api/v2/cortex/analyst/message\",\n            {},\n            {},\n            request_body,\n            {},\n            30000,\n        )\n        if resp[\"status\"] < 400:\n            return json.loads(resp[\"content\"])\n        else:\n            # messages.pop()\n            raise Exception(\n                f\"Failed request with status {resp['status']}: {resp}\"\n            )\n    # @instrument\n    def process_message(self, prompt: str) -> None:\n        \"\"\"Processes a message and adds the response to the chat.\"\"\"\n        messages=[]\n        messages.append(\n            {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prompt}]}\n        )\n        with st.chat_message(\"user\"):\n            st.markdown(prompt)\n        with st.chat_message(\"assistant\"):\n            with st.spinner(\"Generating response...\"):\n                # response = \"who had the most rec yards week 10\"\n                response = self.send_message(prompt=prompt)\n                request_id = response[\"request_id\"]\n                content = response[\"message\"][\"content\"]\n                messages.append(\n                    {**response['message'], \"request_id\": request_id}\n                )\n        return self.display_content(content=content, request_id=request_id, prompt = prompt)  # type: ignore[arg-type]\n        # return response\n    \n    # @instrument\n    def display_content(self,\n        content: List[Dict[str, str]],\n        request_id: Optional[str] = None,\n        message_index: Optional[int] = None,\n        prompt: Optional[str] = None,\n    ) -> None:\n        \"\"\"Displays a content item for a message.\"\"\"\n        message_index = message_index or len(prompt)\n        # if request_id:\n            # with st.expander(\"Request ID\", expanded=False):\n                # st.markdown(request_id)\n        for item in content:\n            if item[\"type\"] == \"text\":\n                st.markdown(item[\"text\"])\n            elif item[\"type\"] == \"suggestions\":\n                with st.expander(\"Suggestions\", expanded=True):\n                    for suggestion_index, suggestion in enumerate(item[\"suggestions\"]):\n                        if st.button(suggestion, key=f\"{message_index}_{suggestion_index}\"):\n                            st.session_state.active_suggestion = suggestion\n            elif item[\"type\"] == \"sql\":\n                return self.display_sql(item[\"statement\"])\n\n    # @instrument\n    def display_sql(self, sql: str) -> None:\n        with st.expander(\"SQL Query\", expanded=False):\n            st.code(sql, language=\"sql\")\n        with st.expander(\"Results\", expanded=True):\n            with st.spinner(\"Running SQL...\"):\n                session = get_active_session()\n                df = session.sql(sql).to_pandas()\n                if len(df.index) > 1:\n                    data_tab, line_tab, bar_tab = st.tabs(\n                        [\"Data\", \"Line Chart\", \"Bar Chart\"]\n                    )\n                    data_tab.dataframe(df)\n                    if len(df.columns) > 1:\n                        df = df.set_index(df.columns[0])\n                    with line_tab:\n                        st.line_chart(df)\n                    with bar_tab:\n                        st.bar_chart(df)\n                else:\n                    st.dataframe(df)\n\n        return df\n\nCA = CortexAnalyst(db='TR_MULTI_AGENT', schema='PUBLIC', stage='SEMANTIC', semantic_model_file_path='customer_data.yaml')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a2252f77-2fea-4ee4-b3e5-beb7e9bbefc2",
   "metadata": {
    "language": "python",
    "name": "test_CA",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "CA_response = CA.process_message(prompt='Show me all the products for Wealthsimple')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ca3f9fe5-35bd-4c8d-b6f8-cc75084e29d8",
   "metadata": {
    "language": "python",
    "name": "test_suggested_product_CA"
   },
   "outputs": [],
   "source": "CA_response = CA.process_message(prompt='Which product should I pitch to Welathsimple?')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9ab01bd9-435d-4d62-9e35-5c931321ace9",
   "metadata": {
    "name": "Cortex_Search_Section",
    "collapsed": false
   },
   "source": "## Define Retrieval Service\n### Get up to date product info and customer references on products to inform sales pitches"
  },
  {
   "cell_type": "code",
   "id": "4410f6a5-fa90-4f4c-a0db-a19927b70b3f",
   "metadata": {
    "language": "sql",
    "name": "list_docs"
   },
   "outputs": [],
   "source": "LS @DOCUMENTS",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3ab2c62a-406e-44c3-bc65-2be40cf1e1fd",
   "metadata": {
    "language": "sql",
    "name": "create_doc_table"
   },
   "outputs": [],
   "source": "CREATE TABLE TR_MULTI_AGENT.PUBLIC.DOC_TEXT_TABLE IF NOT EXISTS (relative_path VARCHAR(500), raw_text VARIANT);",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8631a3ab-1c57-4cc8-897c-56878141cd0f",
   "metadata": {
    "language": "sql",
    "name": "parse_pdfs_into_doc_table"
   },
   "outputs": [],
   "source": "INSERT INTO TR_MULTI_AGENT.PUBLIC.DOC_TEXT_TABLE (relative_path, raw_text)\nWITH html_files AS (\n    SELECT DISTINCT\n        METADATA$FILENAME AS relative_path\n    FROM @TR_MULTI_AGENT.PUBLIC.DOCUMENTS\n    -- WHERE METADATA$FILENAME ILIKE '%.pdf'\n    --   -- Exclude files that have already been parsed\n      WHERE METADATA$FILENAME NOT IN (SELECT relative_path FROM DOC_TEXT_TABLE)\n)\nSELECT \n    relative_path,\n    SNOWFLAKE.CORTEX.PARSE_DOCUMENT(\n        '@TR_MULTI_AGENT.PUBLIC.DOCUMENTS',  -- Your stage name\n        relative_path,  -- File path\n        {'mode': 'layout'}  -- Adjust mode as needed ('layout', 'ocr')\n    ) AS raw_text\nFROM html_files;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c35d88d0-db35-4f9a-97d7-76ff410e2f4b",
   "metadata": {
    "language": "sql",
    "name": "chunk_docs"
   },
   "outputs": [],
   "source": "CREATE TABLE IF NOT EXISTS TR_MULTI_AGENT.PUBLIC.DOC_CHUNKS_TABLE AS\nWITH text_chunks AS (\n    SELECT\n        relative_path,\n        SNOWFLAKE.CORTEX.SPLIT_TEXT_RECURSIVE_CHARACTER(\n            raw_text:content::STRING,  -- Extract the 'content' field from the JSON\n            'markdown',    -- Adjust to 'markdown' if needed\n            2000,       -- Adjust chunk size\n            100,       -- Adjust overlap size\n            ['\\n\\n']     -- Adjust separators\n        ) AS chunks\n    FROM TR_MULTI_AGENT.PUBLIC.DOC_TEXT_TABLE\n)\nSELECT\n    relative_path,\n    c.value AS chunk,\n    (relative_path || ' ' || chunk) AS SEARCH_COL\nFROM text_chunks,\nLATERAL FLATTEN(INPUT => chunks) c;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c68854b1-a0ab-4c31-af06-f564c27bc764",
   "metadata": {
    "language": "python",
    "name": "view_chunked_docs_table"
   },
   "outputs": [],
   "source": "chunk_df = session.table('TR_MULTI_AGENT.PUBLIC.DOC_CHUNKS_TABLE')\nchunk_df.show(3)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "74c5e9ec-2b7a-4b9c-945e-4f71edc7b6d9",
   "metadata": {
    "language": "sql",
    "name": "create_search_service"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE CORTEX SEARCH SERVICE TR_MULTI_AGENT.PUBLIC.PRODUCT_INFO_SEARCH\n    ON SEARCH_COL\n    WAREHOUSE = DEFAULT_XS\n    TARGET_LAG = '365 days'\n    AS SELECT \n        RELATIVE_PATH,\n        CHUNK,\n        SEARCH_COL\nFROM TR_MULTI_AGENT.PUBLIC.DOC_CHUNKS_TABLE",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c3b69c81-cdd2-409c-acc0-24c6accf82a8",
   "metadata": {
    "language": "python",
    "name": "test_RAG",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from typing import List\nfrom snowflake.core import Root\nproduct = \"Cortex Search\"\nprompt = f\"Retrieve relevant documentation for {product}\"\n\nroot = Root(session)\ncortex_search_service = (\n    root\n    .databases[\"TR_MULTI_AGENT\"]\n    .schemas[\"PUBLIC\"]\n    .cortex_search_services[\"PRODUCT_INFO_SEARCH\"]\n)\nresp = cortex_search_service.search(\n    query=prompt,\n    columns=[\"SEARCH_COL\"],\n    limit=5)\n    \ntext_chunks = [row[\"SEARCH_COL\"] for row in resp.results] if resp.results else []\n\nLLM_prompt = f\"Can you summarize product benefits, value props, and a customer reference for product - {product} \\\n                using the following contextual information - {text_chunks}\"\nllm_response = complete(\"snowflake-llama-3.3-70b\", LLM_prompt)\n\nllm_response",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8a2205b4-5f6a-4d60-81c0-11a7a8c4e35c",
   "metadata": {
    "language": "python",
    "name": "temp_test_cell"
   },
   "outputs": [],
   "source": "client = \"Wealthsimple\"\n\n# resp = CA.process_message(f\"What products are being used by our client {client}\")\n# products_used = resp.iloc[:,0].to_list()\nproducts_used = session.sql(f\"SELECT PRODUCTS_USED FROM CUSTOMER_DATA WHERE CUSTOMER= '{client}'\").collect()[0][0]\n\nsuggested_product = session.sql(f\"SELECT PROPENSITY_MODEL_SUGGESTED_PRODUCT FROM CUSTOMER_DATA WHERE CUSTOMER= '{client}'\").collect()[0][0]\n\nprompt = f\"Retrieve relevant documentation for {product}\"\n\nroot = Root(session)\ncortex_search_service = (\n    root\n    .databases[\"TR_MULTI_AGENT\"]\n    .schemas[\"PUBLIC\"]\n    .cortex_search_services[\"PRODUCT_INFO_SEARCH\"]\n)\nresp = cortex_search_service.search(\n    query=prompt,\n    columns=[\"SEARCH_COL\"],\n    limit=5)\n\ntext_chunks = [row[\"SEARCH_COL\"] for row in resp.results] if resp.results else []\n\nLLM_prompt = f\"Can you summarize product benefits, value props, and a customer reference for product - {suggested_product} \\\n                using the following contextual information - {text_chunks}\"\nllm_response = complete(\"snowflake-llama-3.3-70b\", LLM_prompt)\n\n\nfull_response = f'''The following products are currently being used by {client} - {products_used} \\n\\n\nOur propensity model has suggested the following product - SUGGESTED PRODUCT: **{suggested_product}** \\n\\n\n{llm_response}'''\n\n\n# full_response\n\nst.write(f\"# Prep Report for Client **{client}**\" + \\\n                       \"\\n\\n ## Products Used \\n\\n\" + full_response)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "define_multi_agent_class",
    "language": "python"
   },
   "outputs": [],
   "source": "from snowflake.cortex import complete \n\nclass Multi_Agent_App:\n\n    # def __init__(self):\n\n    def agent_1_cortex_analyst(self, client: str) -> str:\n        \"\"\"\n        takes client and inserts into predefined prompt and passes to cortex analyst to return list of products client uses today\n        \"\"\"\n        # resp = CA.process_message(f\"What products are being used by our client {client}\")\n        # products_used = resp.iloc[:,0].to_list()\n\n        # Use sql to get list of currently used products and propensity-model-suggested product\n        products_used = session.sql(f\"SELECT PRODUCTS_USED FROM CUSTOMER_DATA WHERE CUSTOMER= '{client}'\").collect()[0][0]\n        suggested_product = session.sql(f\"SELECT PROPENSITY_MODEL_SUGGESTED_PRODUCT FROM CUSTOMER_DATA WHERE CUSTOMER= '{client}'\").collect()[0][0]\n\n        #Retrieve relevant docuemntation for this product\n        prompt = f\"Retrieve relevant documentation for {product}\"\n        root = Root(session)\n        cortex_search_service = (\n            root\n            .databases[\"TR_MULTI_AGENT\"]\n            .schemas[\"PUBLIC\"]\n            .cortex_search_services[\"PRODUCT_INFO_SEARCH\"]\n        )\n        resp = cortex_search_service.search(\n            query=prompt,\n            columns=[\"SEARCH_COL\"],\n            limit=5)\n    \n        text_chunks = [row[\"SEARCH_COL\"] for row in resp.results] if resp.results else []\n\n        #Pass document chunks to LLM to get a product value prop for suggested product\n        LLM_prompt = f\"Can you summarize product benefits, value props, and a customer reference for product - {suggested_product} \\\n                        using the following contextual information - {text_chunks}\"\n        llm_response = complete(\"snowflake-llama-3.3-70b\", LLM_prompt)\n        \n\n        full_response = f'''The following products are used by {client} - {products_used} \\n\\n\n        \n        \\n Our propensity model has suggested the following product - SUGGESTED PRODUCT: **{suggested_product}** \\n\\n\n        \n        \\n {llm_response}'''\n\n        return full_response\n\n    def agent_2_client_web_search(self, client: str) -> list:\n        \"\"\" \n        calls api to google client name and return top three news articles. \n        Potentially use bs4 or similar library to parse web results\n        Return list of web page text (or potentially pass to LLM to generate summary)\n        \"\"\"\n\n        # api_key = os.environ.get(\"GOOGLE_API_KEY\")\n        # cse_id = os.environ.get(\"GOOGLE_CSE_ID\")\n\n        # web_query = f\"top news for {client}\"\n        # url = \"https://www.googleapis.com/customsearch/v1\"\n        # params = {\n        #     \"key\": api_key,\n        #     \"cx\": cse_id,\n        #     \"q\": web_query,\n        #     \"num\": 3\n        # }\n\n        # top_3_results = requests.get(url, params=params)\n\n        # news_summary_list = []\n\n        # for result in top_3_result:\n        #     news_summaries_list.append(complete('snowflake-llama-3.3-70b', f'summarize the following web page {result}'))\n\n        # return news_summaries_list\n        news_summaries_list = []\n        news_summaries_list.append(complete('llama3.2-1b', f'generate a brief news summary for client {client}. \\\n        It can be fictious but should be within the realm of what that client actually does. \\\n        If it is not a known client you can just make something up that seems reasonable for a tech company'))\n        return news_summaries_list\n\n\n    def agent_3_market_web_search(self, client: str) -> list:\n        \"\"\" \n        uses an llm to determine client industry, then ues api to google client industry and return top three articles. \n        Potentially use bs4 or similar library to parse web results\n        Return list of web page text (or potentially pass to LLM to generate summary)\n        \"\"\"\n        # api_key = os.environ.get(\"GOOGLE_API_KEY\")\n        # cse_id = os.environ.get(\"GOOGLE_CSE_ID\")\n\n        # client_market = complete('snowflake-llama-3.3-70b', f'what is the most applicable market for the company {client}')\n\n        # web_query = f\"market news for {client_market}\"\n        # url = \"https://www.googleapis.com/customsearch/v1\"\n        # params = {\n        #     \"key\": api_key,\n        #     \"cx\": cse_id,\n        #     \"q\": web_query,\n        #     \"num\": 3\n        # }\n\n        # top_3_results = requests.get(url, params=params)\n\n        # market_news_summary_list = []\n\n        # for result in top_3_result:\n        #     market_news_summary_list.append(complete('snowflake-llama-3.3-70b', f'summarize the following web page {result}'))\n        market_news_summaries_list = []\n        market_news_summaries_list.append(complete('llama3.2-1b', f'generate a brief market news summary for the market that client {client} operates in. \\\n        It can be fictious but should be within the realm of how that market actually operates. \\\n        If it is not a known client/market you can just assume the client operates in the information technology market'))\n        return market_news_summaries_list\n\n    def agent_4_prep_pitch(self, client: str, product_info: Optional[str], client_news: Optional[list], market_news: Optional[list]) -> str:\n        \"\"\" \n        passes products, client news, and market news to an LLM to come up with a rough prep pitch\n\n        \"\"\"\n        \n\n        if product_info is None:\n            product_info = self.agent_1_cortex_analyst(client)\n        if client_news is None:\n            client_news = self.agent_2_client_web_search(client)\n        if market_news is None:\n            market_news = self.agent_3_market_web_search(client)\n        prep_pitch = complete('snowflake-llama-3.3-70b', \n                              f\"\"\"\n                              prepare a product pitch for ONLY FOR THE SUGGESTED PRODUCT based on the following product info and the client and market news \n                              product_info: {product_info}\n                              client_news: {client_news}\n                              market_news: {market_news}\n                              \"\"\")\n        return prep_pitch\n    \n    def agent_5_disco_questions(self, client: str, product_info: Optional[str], client_news: Optional[list], market_news: Optional[list]) -> str:\n        \"\"\" \n        passes products, client news, and market news to an LLM to come up with a list of appropriate discovery questions\n        \"\"\"\n        \n        if product_info is None:\n            product_info = self.agent_1_cortex_analyst(client)\n        if client_news is None:\n            client_news = self.agent_2_client_web_search(client)\n        if market_news is None:\n            market_news = self.agent_3_market_web_search(client)\n        disco_questions = complete('snowflake-llama-3.3-70b', \n                              f\"\"\"\n                              prepare a set of discovery questions for a client meeting for the SUGGESTED PRODUCT and the client and market news \n                              product_info: {product_info}\n                              client_news: {client_news}\n                              market_news: {market_news}\n                              \"\"\")\n        return disco_questions\n\n\n    def orchestrate_all_agents(self, client: str, output_file_name: str):\n        \"\"\" \n        primary function that ties all agents together and writes out a final markdown file with all prep materials\n        \"\"\"\n        print(\"Starting Sales Prep Agent Pipeline...\")\n        \n        product_info = self.agent_1_cortex_analyst(client)\n        print(\"Collected Product Info\")\n        \n        client_news = self.agent_2_client_web_search(client)\n        print(\"Collected Client News\")\n\n        market_news = self.agent_3_market_web_search(client)\n        print(\"Collected Market News\")\n\n        pitch = self.agent_4_prep_pitch(client, product_info, client_news, market_news)\n        print(\"Generated Pitch\")\n        \n        disco_questions = self.agent_5_disco_questions(client, product_info, client_news, market_news)\n        print(\"Generated Disco Questions\")\n        \n        markdown_doc = f\"# Prep Report for Client **{client}**\" + \\\n                       \"\\n\\n ## Product Info \\n\\n\" + product_info + \\\n                       \"\\n\\n ## Client News \\n\\n \" + ' '.join(client_news)+ \\\n                       \"\\n\\n ## Market News \\n\\n \" + ' '.join(market_news) + \\\n                       \"\\n\\n ## Proposed Pitch \\n\\n \" + pitch + \\\n                       \"\\n\\n ## Discovery Questions \\n\\n  \" + disco_questions + \\\n                       \"\\n\\n # END OF DOCUMENT - good luck with your meeting :)\"\n\n        with open(output_file_name, \"wb\") as f:\n            f.write(markdown_doc.encode(\"utf-8\"))\n\n        print(f\"Client prep file written to {output_file_name}!\")\n\n        return markdown_doc\n        # , products, client_news, market_news, pitch, disco_questions",
   "id": "ce110000-1111-2222-3333-ffffff000001"
  },
  {
   "cell_type": "code",
   "id": "31dd7eb8-6713-4301-a24d-7201d21ef367",
   "metadata": {
    "language": "python",
    "name": "call_mac"
   },
   "outputs": [],
   "source": "test_mac = Multi_Agent_App()\ntest_md = test_mac.orchestrate_all_agents('Wealthsimple', 'test.md')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5be6919f-1b57-4a31-9277-eb36d4ca9906",
   "metadata": {
    "language": "python",
    "name": "write_out_markdown_doc"
   },
   "outputs": [],
   "source": "st.write(test_md)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b42d8e40-0c12-4358-a684-703e630159fe",
   "metadata": {
    "language": "python",
    "name": "download_report"
   },
   "outputs": [],
   "source": "download_button = st.download_button(f'Download Client Prep Report', open('test.md', 'r'), \n                                     file_name='test.md', use_container_width=True)\nif download_button:\n#    st.session_state.download_clicked = True  # Set flag in session state\n#    if st.session_state.download_clicked:\n    st.success(\"✅ File Downloaded!\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dbc06e75-5fe1-456e-951b-33861c3693ed",
   "metadata": {
    "name": "ARCHIVE_MD",
    "collapsed": false
   },
   "source": "# Archive\n"
  },
  {
   "cell_type": "code",
   "id": "8ad2d8ac-d298-483b-8686-2c61c1fa2607",
   "metadata": {
    "language": "sql",
    "name": "create_network_rule_and_EAI",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "create or replace network rule CLIENT_RESEARCH_NR\n  TYPE = 'HOST_PORT'\n  MODE= 'EGRESS'\n  VALUE_LIST = ('www.businessinsider.com', 'www.techcrunch.com', 'www.google.com', 'www.googleapis.com', 'docs.snowflake.com');\n\nCREATE or replace EXTERNAL ACCESS INTEGRATION CLIENT_RESEARCH_EAI\n  ALLOWED_NETWORK_RULES = (CLIENT_RESEARCH_NR)\n  ENABLED = true;\n\n-- Make sure you enable this external access integration for the notebook!",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6fa3d89a-afe5-49ca-92ba-ba6aa4571db4",
   "metadata": {
    "language": "python",
    "name": "cell2"
   },
   "outputs": [],
   "source": "import requests\nfrom bs4 import BeautifulSoup\n\ndef google_search(query, num_results=10):\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n    }\n    \n    # Format query for Google Search URL\n    search_url = f\"https://www.google.com/search?q={query.replace(' ', '+')}\"\n    \n    # Fetch the page\n    response = requests.get(search_url, headers=headers)\n    \n    if response.status_code != 200:\n        print(f\"Failed to fetch Google search results: {response.status_code}\")\n        return []\n    \n    soup = BeautifulSoup(response.text, \"html.parser\")\n    \n    # Extract search result links\n    links = []\n    \n    for g in soup.find_all(\"a\", href=True):\n        href = g[\"href\"]\n        print(href)\n        if href.startswith(\"/url?q=\"):  # Google uses \"/url?q=\" before the actual link\n            link = href.split(\"&\")[0].replace(\"/url?q=\", \"\")  # Clean up URL\n            links.append(link)\n    \n    return links[:num_results]\n\n# Example usage\nresults = google_search(\"Snowflake Inc\")\nprint(results)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "94645869-8510-4249-83b3-fc22b8b8b4c1",
   "metadata": {
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": "import requests\nfrom bs4 import BeautifulSoup\nr = requests.get(\"https://www.businessinsider.com/answers#snowflake\")\n# r = requests.get(\"https://www.techcrunch.com/?s=snowflake\")\n# r = requests.get(\"https://www.google.com/search?q=snowflake&oq=snowflake\")\n\nsoup = BeautifulSoup(r.text, 'html.parser')\n\n# Find all <a> tags with href attributes\nlinks = soup.find_all('a', href=True)\n\n# Extract and print the links\nfor link in links:\n    url = link['href']\n    text = link.get_text(strip=True)\n    if 'snowflake' in text.lower():\n        print(f\"URL: {url}\")\n        print(f\"Text: {text}\")\n        print(\"---\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a043147d-b688-4f28-b0ab-62a9867d80d5",
   "metadata": {
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": "from googlesearch import search\n\ndef get_top_business_insider_links(search_term):\n    \"\"\"\n    Performs a Google search for a term with site:businessinsider.com and returns the top 3 links.\n\n    Args:\n        search_term (str): The term to search for.\n\n    Returns:\n        list: A list of the top 3 Business Insider links, or an empty list if none found.\n    \"\"\"\n    try:\n        query = f\"{search_term} site:businessinsider.com\"\n        search_results = list(search(query, num_results=3, stop=3, pause=2)) #pause added\n\n        business_insider_links = []\n        for url in search_results:\n            if \"businessinsider.com\" in url:\n                business_insider_links.append(url)\n\n        return business_insider_links\n\n    except Exception as e:\n        print(f\"Error during search: {e}\")\n        return []\n\n# Example usage:\nsearch_term = \"Snowflake\"\ntop_links = get_top_business_insider_links(search_term)\n\nif top_links:\n    for i, link in enumerate(top_links):\n        print(f\"Link {i+1}: {link}\")\nelse:\n    print(\"No Business Insider links found.\")\n\nsearch_term = \"Tesla\"\ntop_links = get_top_business_insider_links(search_term)\n\nif top_links:\n    for i, link in enumerate(top_links):\n        print(f\"Link {i+1}: {link}\")\nelse:\n    print(\"No Business Insider links found.\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6b07e914-b245-4edf-8814-347399eea625",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "outputs": [],
   "source": "# from googlesearch import search\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef get_top_article_text(company_name):\n    \"\"\"\n    Performs a Google search for a company name and returns the raw text of the\n    top three articles.\n\n    Args:\n        company_name (str): The name of the company to search for.\n\n    Returns:\n        list: A list of strings, where each string is the raw text of an article,\n              or an empty list if an error occurs.\n    \"\"\"\n    try:\n\n        article_texts = []\n        for url in search_results:\n            try:\n                response = requests.get(url, timeout=10) #added timeout\n                response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)\n                soup = BeautifulSoup(response.content, 'html.parser')\n                text = soup.get_text(separator=' ', strip=True) #added separator and strip\n                article_texts.append(text)\n            except requests.exceptions.RequestException as e:\n                print(f\"Error fetching URL {url}: {e}\")\n            except Exception as e:\n                print(f\"Error processing URL {url}: {e}\")\n\n        return article_texts\n\n    except Exception as e:\n        print(f\"Error during search: {e}\")\n        return []\n\n# Example usage:\ncompany = \"Snowflake\"\narticle_texts = get_top_article_text(company)\n\nif article_texts:\n    for i, text in enumerate(article_texts):\n        print(f\"Article {i+1}:\\n{text}\\n{'-'*40}\\n\")\nelse:\n    print(\"Could not retrieve article text.\")\n\n\n\n\n\n\nweb_query = f\"top news for Snowflake\"\nurl = \"https://www.googleapis.com/customsearch/v1\"\nparams = {\n    \"key\": '',\n    \"cx\": '',\n    \"q\": web_query,\n    \"num\": 3\n}\n\ntop_3_results = requests.get(url, params=params)\ntop_3_results",
   "execution_count": null
  }
 ]
}